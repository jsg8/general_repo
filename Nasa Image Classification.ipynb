{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import nasapy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import nasa dataset for model training\n",
    "# Credit to Data Science Student Society at UC San Diego\n",
    "\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "train_path = r\"...\\train-calibrated-shuffled.txt\"\n",
    "validation_path = r\"...\\val-calibrated-shuffled.txt\"\n",
    "test_path = r\"...\\test-calibrated-shuffled.txt\"\n",
    "\n",
    "\n",
    "def PathtoDataFrame(path):\n",
    "    \"\"\"Import Data from its File Location and Convert to a DataFrame.\n",
    "        Args:\n",
    "            path: File Path of Data\n",
    "            \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(pd.read_csv(path, sep = \" \"))\n",
    "    df[2] = \"'...initial path location'\n",
    "    df.columns = [\"a\",\"Class\",\"c\"]\n",
    "    df = df[['c','a','Class']]\n",
    "    df[\"Paths\"] = df[[\"c\", \"a\"]].apply(\"\".join, axis=1)\n",
    "    for i,j in enumerate(df['Paths']):\n",
    "        df['Paths'][i] = df['Paths'][i].replace('/','\\\\')\n",
    "    df = df[['Class', 'Paths']]\n",
    "    return df\n",
    "\n",
    "train_df = PathtoDataFrame(train_path)\n",
    "validation_df = PathtoDataFrame(validation_path)\n",
    "test_df = PathtoDataFrame(test_path)\n",
    "\n",
    "full_train_df = pd.concat([train_df, validation_df], axis = 0, ignore_index = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "# Reclassify Images for Model Input\n",
    "\n",
    "class_labels = {0: 'other', 1: 'other', 2: 'other', 3: 'chemin inlet open', 4: 'other',\n",
    "                         5: 'drill holes', 6: 'other', 7: 'other', 8: 'ground', 9: 'horizon', 10: 'inlet',\n",
    "                         11: 'other', 12: 'other', 13: 'other', 14: 'mastcam cal target',\n",
    "                         15: 'observation tray', 16: 'other', 17: 'portion tube', 18: 'other',\n",
    "                         19: 'other', 20: 'other', 21: 'scoop', 22: 'sun', 23: 'turret', 24: 'wheel'}\n",
    "\n",
    "new_class_labels = {'chemin inlet open': 0,\n",
    "                    'drill holes': 1, 'ground': 2, 'horizon': 3, 'inlet': 4,\n",
    "                    'mastcam cal target': 5, 'observation tray': 6, 'portion tube': 7,\n",
    "                    'scoop': 8,  'turret': 9, 'wheel': 10, 'other': 11}\n",
    "def classifier(series):\n",
    "    \"\"\"Convert Original Classes into the Classes used for Analysis.\n",
    "        Args:\n",
    "            series: Series of Image Classes\n",
    "            \"\"\"\n",
    "    for ind, item in series.items():\n",
    "        item = series.replace(item, class_labels[item], inplace = True)\n",
    "    for ind, item in series.items():\n",
    "        if item in new_class_labels.keys():\n",
    "            item = series.replace(item, new_class_labels[item], inplace = True)\n",
    "\n",
    "X_train, Y_train = full_train_df['Paths'], full_train_df['Class']\n",
    "X_test, Y_test = test_df['Paths'], test_df['Class']\n",
    "\n",
    "classifier(Y_train)\n",
    "classifier(Y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "# Filter Data to Remove Uncommon Classes and Reduce Class Imbalance\n",
    "\n",
    "# Separate Class 2: Ground and Remove 2/3 of Ground Observations\n",
    "X_train_2s = X_train[Y_train == 2]\n",
    "X_train_no_2s = X_train[Y_train != 2]\n",
    "Y_train_2s = Y_train[Y_train == 2]\n",
    "Y_train_no_2s = Y_train[Y_train != 2]\n",
    "\n",
    "X_train_1 = X_train[Y_train_2s][:int(len(Y_train_2s)/3)]\n",
    "Y_train_1 = Y_train[Y_train_2s][:int(len(Y_train_2s)/3)]\n",
    "\n",
    "# Remove Class 11: Other\n",
    "X_train_2 = X_train_no_2s[Y_train != 11]\n",
    "Y_train_2 = Y_train_no_2s[Y_train != 11]\n",
    "\n",
    "X_test = X_test[Y_test != 11]\n",
    "Y_test = Y_test[Y_test != 11]\n",
    "\n",
    "# Recombine Filtered Dataset\n",
    "X_train = pd.concat([X_train_1, X_train_2], axis = 0, ignore_index = True)\n",
    "Y_train = pd.concat([Y_train_1, Y_train_2], axis = 0, ignore_index = True)\n",
    "\n",
    "# Randomly Shuffle the Training Data\n",
    "Y_train_final = Y_train.sample(frac = 1)\n",
    "idx = Y_train_final.index\n",
    "X_train_final = X_train.reindex(idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [],
   "source": [
    "# Create TF Dataset that is Usable for Model Input and Training\n",
    "\n",
    "batch_size = 32\n",
    "buffer_size = 32\n",
    "\n",
    "def read_image_scale(img_path, label):\n",
    "    \"\"\"Import Images from their File Location and Scale them.\n",
    "        Args:\n",
    "            img_path: Path to Images\n",
    "            label: Corresponding Image Labels\n",
    "            \"\"\"\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    img = img/255\n",
    "    return img, label\n",
    "\n",
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(read_image_scale, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "\n",
    "    if is_training == True:\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(buffer_size = buffer_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_images = create_dataset(X_train_final, Y_train_final)\n",
    "test_images = create_dataset(X_test, Y_test, is_training = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create initial model: init_model\n",
    "\n",
    "init_model = tf.keras.applications.resnet_v2.ResNet50V2(include_top = False, input_shape = (256, 256, 3))\n",
    "\n",
    "n_classes = 11\n",
    "n_epochs = 10\n",
    "\n",
    "data_aug = tf.keras.Sequential([tf.keras.layers.RandomFlip(mode = 'horizontal'), tf.keras.layers.RandomRotation(factor = 0.3), tf.keras.layers.RandomZoom(height_factor = (-0.3, 0.3))])\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = [256, 256, 3])\n",
    "\n",
    "x = data_aug(inputs)\n",
    "\n",
    "x = init_model(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "layer1 = tf.keras.layers.Dropout(.2)(x)\n",
    "layer1 = tf.keras.layers.Dense(128, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l2(0.01), bias_regularizer = tf.keras.regularizers.l2(0.01))(layer1)\n",
    "outputs = tf.keras.layers.Dense(n_classes, activation = 'softmax')(layer1)\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "# Compile model\n",
    "\n",
    "learning_rate = 0.0001\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate), loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False), metrics = ['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(train_images, epochs = n_epochs, validation_data = test_images)\n",
    "model.save('init_model_fin.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Validate Model on Testing Data\n",
    "\n",
    "model.evaluate(test_images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get curiosity rover data for past 1000 sols as of 8/8/2022\n",
    "# greatest amount available to pull using nasa api\n",
    "\n",
    "key = 'UAlPWPyCNSbK9PVLVBhwCGDlnjL6cyYjfiMEgle7'\n",
    "nasa = nasapy.Nasa(key = key)\n",
    "base_sol = 3558\n",
    "\n",
    "curiosity_data = []\n",
    "\n",
    "for sol in range(1000):\n",
    "    curiosity = nasa.mars_rover(sol = base_sol - sol)\n",
    "    curiosity_data.append(curiosity)\n",
    "\n",
    "# convert data into usable dataframe: curiosity_fin\n",
    "\n",
    "curiosity_inter = pd.DataFrame(curiosity_data)\n",
    "filler_list = []\n",
    "\n",
    "for col in curiosity_inter.columns:\n",
    "    for ind in curiosity_inter.index:\n",
    "        a = curiosity_inter.iloc[ind, col]\n",
    "        b = pd.DataFrame(a)\n",
    "        filler_list.append(b)\n",
    "curiosity_fin = pd.concat(filler_list, ignore_index = True)\n",
    "curiosity_fin = curiosity_fin.drop_duplicates(subset = 'img_src')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Create TF Dataset that is usable for Model Prediction\n",
    "\n",
    "def read_image_url(url):\n",
    "    \"\"\"Create Tensor from Image URL.\n",
    "        Args:\n",
    "            url: Image URL\n",
    "            \"\"\"\n",
    "    url = tf.keras.utils.get_file(fname = 'file', origin = url)\n",
    "    img = tf.io.read_file(url)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    img = img / 255\n",
    "    return img\n",
    "\n",
    "def create_dataset_from_url(filenames, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image URLs\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames))\n",
    "    dataset = dataset.map(read_image_url, num_parallel_calls = tf.data.AUTOTUNE)\n",
    "\n",
    "    if is_training == True:\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(buffer_size = buffer_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "curiosity_images = create_dataset_from_url(curiosity_fin['img_src'], is_training = False)\n",
    "\n",
    "# Predict the Classes of the Newest Images\n",
    "model.predict(curiosity_images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
